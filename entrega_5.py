# -*- coding: utf-8 -*-
"""Entrega_5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Fww12rTzn1NumGgQeOMxCc-js3O8cD7k

_Laura Martínez González de Aledo_

Enero 2021

## 1. The World Bank's international debt data
<p>No es que los humanos solo tengamos deudas para administrar nuestras necesidades. Un país también puede endeudarse para administrar su economía. Por ejemplo, el gasto en infraestructura es un ingrediente costoso requerido para que los ciudadanos de un país lleven una vida cómoda. El Banco Mundial es la organización que proporciona deuda a los países.</p>

<!-- <p>En este notebook, vamos a analizar los datos de la deuda internacional recopilados por el Banco Mundial. El conjunto de datos contiene información sobre el monto de la deuda (en USD) que deben los países en desarrollo en varias categorías.</p>  -->

#### **1. Inicializar y cargar el contexto spark**
"""

!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://www-us.apache.org/dist/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz
!tar xf spark-2.4.7-bin-hadoop2.7.tgz 
!pip install -q findspark 

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64" 
os.environ["SPARK_HOME"] = "/content/spark-2.4.7-bin-hadoop2.7"

import findspark
# Inicio findspark 
findspark.init()  

# Inicio una sesion en Spark 
from pyspark import SparkContext
# y creo un objeto RDD para comenzar a trabajar con Spark DataFrames
sc = SparkContext.getOrCreate() 

# Importo SparkSession desde pyspark sql
from pyspark.sql import SparkSession
# Creo un objeto que se llame Spark
spark = SparkSession.builder.master("local[*]").getOrCreate()

"""### Cargamos los datos:

* Cargamos solo los archivos de _indicadores de desarrollo_ y _indicadores de deuda_ por que son los que necesitamos para realizar la práctica.
"""

indicadores_desarrollo = sc.textFile("./indicadores_desarrollo.csv")
indicadores_desarrollo.take(2)

indicadores_deuda = sc.textFile("./indicadores_deuda.csv")
indicadores_deuda.take(2)

"""### Parseamos los datos:

* El primer paso sería quitar la cabecera  (nos orientaremos con los archivos de explicación)
"""

header = indicadores_desarrollo.first()
indicadores_desarrollo_wh = indicadores_desarrollo.filter(lambda x: x != header)

header = indicadores_deuda.first()
indicadores_deuda_wh = indicadores_deuda.filter(lambda x: x != header)

"""* Vemos cuál es separador de las columnas:"""

desarrollo_split = indicadores_desarrollo_wh.map(lambda x: x.split(','))

deuda_split = indicadores_deuda_wh.map(lambda x: x.split(','))

"""* Creamos una función para pasar a float los NAs:"""

def tofloat (x):
  if x in ["" , " " , "nan" , "null"]:
    output = 0.0
  else:
    output = float(x)
  return output

"""* Creamos una función para ajustar los países cuyo nombre se compone de dos palabras, a la longitud de las demás filas:"""

def parsed2(line,lon):
    line_splitted = line.split(",")
    if (len(line_splitted) == lon): 
        l = line_splitted
    else:
        l = line_splitted[2:]
        l.insert(0, line_splitted[0]+line_splitted[1])
    return l

desarrollo = indicadores_desarrollo_wh.map(lambda x: parsed2(x,6)).map(lambda x: len(x)).distinct() 
deuda = indicadores_deuda_wh.map(lambda x: parsed2(x,27)).map(lambda x: len(x)).distinct()

"""##### **2. Número de países distintos en cada dataset. Coinciden?**"""

desarrollo_split.map(lambda x: x[0]).distinct().count() 
# con .colletc() en vez de .count() muestra la lista de paises

deuda_split.map(lambda x: x[0]).distinct().count()

"""##### **3. Total de deuda contraida por cada pais: total amount of debt (in USD) DT.AMT.MLAT.CD**"""

deuda_split.map(lambda x: (x[0], x[5])).collect()

"""##### **4. Media de los indicadores de deuda (tabla uno): DT.AMT.BLAT.CD, DT.DIS.BLAT.CD, DT.INT.BLAT.CD**"""

from pyspark.mllib.stat import Statistics #Archivo Cunef7

deuda_split.map(lambda x: (x[0],x[2],x[11],x[18])).collect()

"""##### **5. Los 20 paises con DT.AMT.DLXF.CD más alto**"""

h = indicadores_deuda_wh.first()
# 27 es el numero de columnas
indicadores_deuda_wh.filter(lambda x: x != h).map(lambda x: parsed2(x,27)).map(lambda x: (x[0],x[3])).\
takeOrdered(20,lambda x: -float(x[1])) #-float para que me lo ordene de forma descendente y me quede el maximo el primero

"""##### **6. Pais con los datos informados todos los años.**"""

desarrollo_split.map(lambda x: (x[0], x[2])).sortBy(lambda x: x[0], ascending = False).distinct()

pais_informado = desarrollo_split.filter(lambda x: "2017" in x[2]).map(lambda x: (x[0]))
pais_informado.collect()

"""##### **7. Media anual de los distintos indicadores de desarrollo**"""

import pyspark.sql 
from pyspark.sql import SQLContext, functions as F, Row
from pyspark.sql.types import *
from pyspark.sql.types import StringType, IntegerType
import pandas as pd

datos = pd.read_csv('indicadores_desarrollo.csv', sep =',', names=["country_name", "country_code","anho","GC_DOD_TOTL_CN","GC_DOD_TOTL_GD_ZS", "SP_DYN_CBRT_IN"], skiprows=1)

sqlContext = SQLContext(sc)
df_desarrollo = sqlContext.createDataFrame(datos)

df_desarrollo.groupby('anho').agg(F.mean('GC_DOD_TOTL_CN')).show()

df_desarrollo.groupby('anho').agg(F.mean('GC_DOD_TOTL_GD_ZS')).show()

df_desarrollo.groupby('anho').agg(F.mean('SP_DYN_CBRT_IN')).show()

"""##### **8. Podrías decirme el total de deuda acumulada DT.AMT.MLAT.CD por los 10 países con un valor en media menor de SP.DYN.CBRT.IN**"""

from pyspark.sql.functions import col, avg, desc, sum

df_desarrollo = pd.read_csv(indicadores_desarrollo)
desarrollo_df = sqlContext.createDataFrame(df_desarrollo)
desarrollo_df.show(3)

df_deuda = pd.read_csv(indicadores_deuda)
deuda_df = sqlContext.createDataFrame(df_deuda)
deuda_df.show(3)

union = desarrollo_df.join(other=deuda_df, on=["country_name"], how="inner")
union.show(3)

df = union.withColumnRenamed("anho", "year") \
    .withColumnRenamed("DT.AMT.MLAT.CD", "deuda") \
    .withColumnRenamed("SP.DYN.CBRT.IN", "birthdate")

colum = df.select("country_name","deuda","birthdate")
colum.show()

total = colum.groupBy("country_name", "deuda").agg(avg("birthdate"))
total.show()

total = total.sort("avg(birthdate)").limit(10)
total.show()

total.select(sum("deuda")).show()

"""##### **9. ¿Hay alguna relación entre los nacimientos y el indicador DT.AMT.DLXF.CD? ¿Cómo podrías demostrarlo?**"""

from pyspark.ml.stat import Correlation

renom = df.withColumnRenamed("DT.AMT.DLXF.CD", "deuda_dlxf")

select = renom.select("birthdate","deuda_dlxf")
select.show()

select.stat.corr("deuda_dlxf", "birthdate")

sc.stop()